
import Jobs.Extractor.CommitRecord.{extractLanguage, extractPackages, extractDate}
import org.scalatest.FunSuite

class CommitRecordTestSuite extends FunSuite {
  // placeholder for unit testing.
  trait TestJsonData {
    val testJson = """{ "_id" : ObjectId( "5adfe2136480fd20ca9fc2c3" ), "sha" : "d4479ae679dd4b8695e6fad3ac2657d1e94958af", "commit" : { "author" : { "name" : "wchen129", "email" : "WChen129@its.jnj.com", "date" : "2018-04-25T02:03:13Z" }, "committer" : { "name" : "wchen129", "email" : "WChen129@its.jnj.com", "date" : "2018-04-25T02:03:13Z" }, "message" : "spark sql", "tree" : { "sha" : "fef4c65025125d47dfdd52d1135b9da4d52d0337", "url" : "https://api.github.com/repos/haha174/spark/git/trees/fef4c65025125d47dfdd52d1135b9da4d52d0337" }, "url" : "https://api.github.com/repos/haha174/spark/git/commits/d4479ae679dd4b8695e6fad3ac2657d1e94958af", "comment_count" : 0, "verification" : { "verified" : false, "reason" : "unsigned", "signature" : null, "payload" : null } }, "url" : "https://api.github.com/repos/haha174/spark/commits/d4479ae679dd4b8695e6fad3ac2657d1e94958af", "html_url" : "https://github.com/haha174/spark/commit/d4479ae679dd4b8695e6fad3ac2657d1e94958af", "comments_url" : "https://api.github.com/repos/haha174/spark/commits/d4479ae679dd4b8695e6fad3ac2657d1e94958af/comments", "author" : null, "committer" : null, "parents" : [ { "sha" : "8fb03c2f9d84f732ec11ffed9987634188d47e7d", "url" : "https://api.github.com/repos/haha174/spark/commits/8fb03c2f9d84f732ec11ffed9987634188d47e7d", "html_url" : "https://github.com/haha174/spark/commit/8fb03c2f9d84f732ec11ffed9987634188d47e7d" } ], "stats" : { "total" : 701, "additions" : 674, "deletions" : 27 }, "files" : [ { "sha" : "a8788263b2cfaf6a24ba193aa47231f9d09b2907", "filename" : "spark-scala-line-action/src/main/java/com/wen/spark/scala/submit/WorldCountLocal.scala", "status" : "modified", "additions" : 12, "deletions" : 2, "changes" : 14, "blob_url" : "https://github.com/haha174/spark/blob/d4479ae679dd4b8695e6fad3ac2657d1e94958af/spark-scala-line-action/src/main/java/com/wen/spark/scala/submit/WorldCountLocal.scala", "raw_url" : "https://github.com/haha174/spark/raw/d4479ae679dd4b8695e6fad3ac2657d1e94958af/spark-scala-line-action/src/main/java/com/wen/spark/scala/submit/WorldCountLocal.scala", "contents_url" : "https://api.github.com/repos/haha174/spark/contents/spark-scala-line-action/src/main/java/com/wen/spark/scala/submit/WorldCountLocal.scala?ref=d4479ae679dd4b8695e6fad3ac2657d1e94958af", "patch" : "@@ -8,11 +8,21 @@ object SparkOnFile {\n     val sc = new SparkContext(sparkConf);\n \n     //val textFile = sc.textFile(\"hdfs://spark1:8020/world-count.txt\")\n-    val textFile = sc.textFile(\"C:\\\\Users\\\\haha174\\\\Desktop\\\\data\\\\world-count.txt\")\n-    val counts = textFile.flatMap(line => line.split(\"-\"))\n+    val textFile = sc.textFile(\"C:\\\\Users\\\\wchen129\\\\Desktop\\\\data\\\\sparkdata\\\\world-count.txt\")\n+    var str=\"haha wo\";\n+    var str1=str.split(\" \");\n+    var str2=str.split(\" \",3);\n+    var str3=str.split(\" \")(0);\n+\n+    str1.foreach(count=>println(\"str1+ \"+count))\n+    str2.foreach(count=>println(\"str2+ \"+count))\n+    str3.foreach(count=>println(\"str3+ \"+count))\n+    println(str3)\n+    val counts = textFile.flatMap(line => line.split(\" \")(0))\n       .map(word => (word, 1))\n       .reduceByKey(_ + _)\n     counts.foreach(count=>println(count._1+\" appeard \"+count._2+\" times\"))\n     //counts.saveAsTextFile(\"hdfs://spark1:8020/world-count-result.txt\")\n+\n   }\n }\n\\ No newline at end of file" }, { "sha" : "0b969f675af915b0f1004111776b990478321f9a", "filename" : "spark-study-sql-java/pom.xml", "status" : "modified", "additions" : 6, "deletions" : 0, "changes" : 6, "blob_url" : "https://github.com/haha174/spark/blob/d4479ae679dd4b8695e6fad3ac2657d1e94958af/spark-study-sql-java/pom.xml", "raw_url" : "https://github.com/haha174/spark/raw/d4479ae679dd4b8695e6fad3ac2657d1e94958af/spark-study-sql-java/pom.xml", "contents_url" : "https://api.github.com/repos/haha174/spark/contents/spark-study-sql-java/pom.xml?ref=d4479ae679dd4b8695e6fad3ac2657d1e94958af", "patch" : "@@ -62,6 +62,12 @@\n             <artifactId>fastjson</artifactId>\n             <version>1.2.46</version>\n         </dependency>\n+        <dependency>\n+            <groupId>mysql</groupId>\n+            <artifactId>mysql-connector-java</artifactId>\n+            <version>5.1.44</version>\n+            <scope>provided</scope>\n+        </dependency>\n     </dependencies>\n \n     <build>" }, { "sha" : "c317a498ef5c1771d915efd44d4cb6ef8df8c882", "filename" : "spark-study-sql-java/spark-study-sql-java.iml", "status" : "added", "additions" : 217, "deletions" : 0, "changes" : 217, "blob_url" : "https://github.com/haha174/spark/blob/d4479ae679dd4b8695e6fad3ac2657d1e94958af/spark-study-sql-java/spark-study-sql-java.iml", "raw_url" : "https://github.com/haha174/spark/raw/d4479ae679dd4b8695e6fad3ac2657d1e94958af/spark-study-sql-java/spark-study-sql-java.iml", "contents_url" : "https://api.github.com/repos/haha174/spark/contents/spark-study-sql-java/spark-study-sql-java.iml?ref=d4479ae679dd4b8695e6fad3ac2657d1e94958af", "patch" : "@@ -0,0 +1,217 @@\n+<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n+<module org.jetbrains.idea.maven.project.MavenProjectsManager.isMavenModule=\"true\" type=\"JAVA_MODULE\" version=\"4\">\n+  <component name=\"NewModuleRootManager\" LANGUAGE_LEVEL=\"JDK_1_8\">\n+    <output url=\"file://$MODULE_DIR$/target/classes\" />\n+    <output-test url=\"file://$MODULE_DIR$/target/test-classes\" />\n+    <content url=\"file://$MODULE_DIR$\">\n+      <sourceFolder url=\"file://$MODULE_DIR$/src/main/java\" isTestSource=\"false\" />\n+      <sourceFolder url=\"file://$MODULE_DIR$/src/main/resources\" type=\"java-resource\" />\n+      <sourceFolder url=\"file://$MODULE_DIR$/src/test/java\" isTestSource=\"true\" />\n+      <excludeFolder url=\"file://$MODULE_DIR$/target\" />\n+    </content>\n+    <orderEntry type=\"inheritedJdk\" />\n+    <orderEntry type=\"sourceFolder\" forTests=\"false\" />\n+    <orderEntry type=\"library\" scope=\"TEST\" name=\"Maven: junit:junit:4.12\" level=\"project\" />\n+    <orderEntry type=\"library\" scope=\"TEST\" name=\"Maven: org.hamcrest:hamcrest-core:1.3\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.apache.spark:spark-core_2.11:2.2.1\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.apache.avro:avro:1.7.7\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.codehaus.jackson:jackson-core-asl:1.9.13\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: com.thoughtworks.paranamer:paranamer:2.3\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.apache.commons:commons-compress:1.4.1\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.tukaani:xz:1.0\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.apache.avro:avro-mapred:hadoop2:1.7.7\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.apache.avro:avro-ipc:1.7.7\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.apache.avro:avro-ipc:tests:1.7.7\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: com.twitter:chill_2.11:0.8.0\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: com.esotericsoftware:kryo-shaded:3.0.3\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: com.esotericsoftware:minlog:1.3.0\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.objenesis:objenesis:2.1\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: com.twitter:chill-java:0.8.0\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.apache.xbean:xbean-asm5-shaded:4.4\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.apache.spark:spark-launcher_2.11:2.2.1\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.apache.spark:spark-network-common_2.11:2.2.1\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.fusesource.leveldbjni:leveldbjni-all:1.8\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: com.fasterxml.jackson.core:jackson-annotations:2.6.5\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.apache.spark:spark-network-shuffle_2.11:2.2.1\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.apache.spark:spark-unsafe_2.11:2.2.1\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: net.java.dev.jets3t:jets3t:0.9.3\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.apache.httpcomponents:httpcore:4.3.3\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: javax.activation:activation:1.1.1\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: mx4j:mx4j:3.0.2\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: javax.mail:mail:1.4.7\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.bouncycastle:bcprov-jdk15on:1.51\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: com.jamesmurty.utils:java-xmlbuilder:1.0\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: net.iharder:base64:2.3.8\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.apache.curator:curator-recipes:2.6.0\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.apache.curator:curator-framework:2.6.0\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.apache.zookeeper:zookeeper:3.4.6\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: com.google.guava:guava:16.0.1\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: javax.servlet:javax.servlet-api:3.1.0\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.apache.commons:commons-lang3:3.5\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.apache.commons:commons-math3:3.4.1\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: com.google.code.findbugs:jsr305:1.3.9\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.slf4j:slf4j-api:1.7.16\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.slf4j:jul-to-slf4j:1.7.16\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.slf4j:jcl-over-slf4j:1.7.16\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: log4j:log4j:1.2.17\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.slf4j:slf4j-log4j12:1.7.16\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: com.ning:compress-lzf:1.0.3\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.xerial.snappy:snappy-java:1.1.2.6\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: net.jpountz.lz4:lz4:1.3.0\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.roaringbitmap:RoaringBitmap:0.5.11\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: commons-net:commons-net:2.2\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.scala-lang:scala-library:2.11.8\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.json4s:json4s-jackson_2.11:3.2.11\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.json4s:json4s-core_2.11:3.2.11\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.json4s:json4s-ast_2.11:3.2.11\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.scala-lang:scalap:2.11.0\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.scala-lang:scala-compiler:2.11.0\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.glassfish.jersey.core:jersey-client:2.22.2\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: javax.ws.rs:javax.ws.rs-api:2.0.1\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.glassfish.hk2:hk2-api:2.4.0-b34\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.glassfish.hk2:hk2-utils:2.4.0-b34\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.glassfish.hk2.external:aopalliance-repackaged:2.4.0-b34\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.glassfish.hk2.external:javax.inject:2.4.0-b34\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.glassfish.hk2:hk2-locator:2.4.0-b34\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.javassist:javassist:3.18.1-GA\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.glassfish.jersey.core:jersey-common:2.22.2\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: javax.annotation:javax.annotation-api:1.2\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.glassfish.jersey.bundles.repackaged:jersey-guava:2.22.2\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.glassfish.hk2:osgi-resource-locator:1.0.1\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.glassfish.jersey.core:jersey-server:2.22.2\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.glassfish.jersey.media:jersey-media-jaxb:2.22.2\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: javax.validation:validation-api:1.1.0.Final\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.glassfish.jersey.containers:jersey-container-servlet:2.22.2\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.glassfish.jersey.containers:jersey-container-servlet-core:2.22.2\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: io.netty:netty-all:4.0.43.Final\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: io.netty:netty:3.9.9.Final\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: com.clearspring.analytics:stream:2.7.0\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: io.dropwizard.metrics:metrics-core:3.1.2\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: io.dropwizard.metrics:metrics-jvm:3.1.2\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: io.dropwizard.metrics:metrics-json:3.1.2\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: io.dropwizard.metrics:metrics-graphite:3.1.2\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: com.fasterxml.jackson.core:jackson-databind:2.6.5\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: com.fasterxml.jackson.core:jackson-core:2.6.5\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: com.fasterxml.jackson.module:jackson-module-scala_2.11:2.6.5\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.scala-lang:scala-reflect:2.11.7\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: com.fasterxml.jackson.module:jackson-module-paranamer:2.6.5\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.apache.ivy:ivy:2.4.0\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: oro:oro:2.0.8\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: net.razorvine:pyrolite:4.13\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: net.sf.py4j:py4j:0.10.4\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.apache.spark:spark-tags_2.11:2.2.1\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.apache.commons:commons-crypto:1.0.0\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.spark-project.spark:unused:1.0.0\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.apache.spark:spark-sql_2.11:2.2.1\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: com.univocity:univocity-parsers:2.2.1\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.apache.spark:spark-sketch_2.11:2.2.1\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.apache.spark:spark-catalyst_2.11:2.2.1\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.codehaus.janino:janino:3.0.0\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.codehaus.janino:commons-compiler:3.0.0\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.antlr:antlr4-runtime:4.5.3\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.apache.parquet:parquet-column:1.8.2\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.apache.parquet:parquet-common:1.8.2\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.apache.parquet:parquet-encoding:1.8.2\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.apache.parquet:parquet-hadoop:1.8.2\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.apache.parquet:parquet-format:2.3.1\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.apache.parquet:parquet-jackson:1.8.2\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.apache.spark:spark-hive_2.11:2.2.1\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: com.twitter:parquet-hadoop-bundle:1.6.0\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.spark-project.hive:hive-exec:1.2.1.spark2\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: commons-io:commons-io:2.4\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: commons-lang:commons-lang:2.6\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: javolution:javolution:5.5.1\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: log4j:apache-log4j-extras:1.2.17\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.antlr:antlr-runtime:3.4\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.antlr:stringtemplate:3.2.1\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: antlr:antlr:2.7.7\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.antlr:ST4:4.0.4\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: com.googlecode.javaewah:JavaEWAH:0.3.2\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.iq80.snappy:snappy:0.2\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: stax:stax-api:1.0.1\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: net.sf.opencsv:opencsv:2.3\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.spark-project.hive:hive-metastore:1.2.1.spark2\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: com.jolbox:bonecp:0.8.0.RELEASE\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: commons-cli:commons-cli:1.2\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: commons-logging:commons-logging:1.1.3\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.datanucleus:datanucleus-api-jdo:3.2.6\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.datanucleus:datanucleus-rdbms:3.2.9\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: commons-pool:commons-pool:1.5.4\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: commons-dbcp:commons-dbcp:1.4\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: javax.jdo:jdo-api:3.0.1\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: javax.transaction:jta:1.1\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: commons-httpclient:commons-httpclient:3.1\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.apache.calcite:calcite-avatica:1.2.0-incubating\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.apache.calcite:calcite-core:1.2.0-incubating\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.apache.calcite:calcite-linq4j:1.2.0-incubating\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: net.hydromatic:eigenbase-properties:1.1.5\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.apache.httpcomponents:httpclient:4.5.2\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.codehaus.jackson:jackson-mapper-asl:1.9.13\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: commons-codec:commons-codec:1.10\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: joda-time:joda-time:2.9.3\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.jodd:jodd-core:3.5.2\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.datanucleus:datanucleus-core:3.2.10\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.apache.thrift:libthrift:0.9.3\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.apache.thrift:libfb303:0.9.3\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.apache.derby:derby:10.12.1.1\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.apache.spark:spark-streaming_2.11:2.2.1\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.apache.hadoop:hadoop-client:2.9.0\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.apache.hadoop:hadoop-common:2.9.0\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: xmlenc:xmlenc:0.52\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: commons-collections:commons-collections:3.2.2\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.mortbay.jetty:jetty-sslengine:6.1.26\" level=\"project\" />\n+    <orderEntry type=\"library\" scope=\"RUNTIME\" name=\"Maven: javax.servlet.jsp:jsp-api:2.1\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: commons-configuration:commons-configuration:1.6\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: commons-digester:commons-digester:1.8\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: commons-beanutils:commons-beanutils:1.7.0\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: commons-beanutils:commons-beanutils-core:1.8.0\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: com.google.protobuf:protobuf-java:2.5.0\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: com.google.code.gson:gson:2.2.4\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.apache.hadoop:hadoop-auth:2.9.0\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: com.nimbusds:nimbus-jose-jwt:3.9\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: net.jcip:jcip-annotations:1.0\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: net.minidev:json-smart:1.1.1\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.apache.directory.server:apacheds-kerberos-codec:2.0.0-M15\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.apache.directory.server:apacheds-i18n:2.0.0-M15\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.apache.directory.api:api-asn1-api:1.0.0-M20\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.apache.directory.api:api-util:1.0.0-M20\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.apache.curator:curator-client:2.7.1\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.apache.htrace:htrace-core4:4.1.0-incubating\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.codehaus.woodstox:stax2-api:3.1.4\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: com.fasterxml.woodstox:woodstox-core:5.0.3\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.apache.hadoop:hadoop-hdfs-client:2.9.0\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: com.squareup.okhttp:okhttp:2.4.0\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: com.squareup.okio:okio:1.4.0\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.apache.hadoop:hadoop-mapreduce-client-app:2.9.0\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.apache.hadoop:hadoop-mapreduce-client-common:2.9.0\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.apache.hadoop:hadoop-mapreduce-client-shuffle:2.9.0\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.apache.hadoop:hadoop-yarn-server-common:2.9.0\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.apache.geronimo.specs:geronimo-jcache_1.0_spec:1.0-alpha-1\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.ehcache:ehcache:3.3.1\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: com.zaxxer:HikariCP-java7:2.4.12\" level=\"project\" />\n+    <orderEntry type=\"library\" scope=\"RUNTIME\" name=\"Maven: com.microsoft.sqlserver:mssql-jdbc:6.2.1.jre7\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.apache.hadoop:hadoop-yarn-api:2.9.0\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: javax.xml.bind:jaxb-api:2.2.2\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: javax.xml.stream:stax-api:1.0-2\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.apache.hadoop:hadoop-mapreduce-client-core:2.9.0\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.apache.hadoop:hadoop-yarn-client:2.9.0\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.apache.hadoop:hadoop-yarn-common:2.9.0\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: javax.servlet:servlet-api:2.5\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.mortbay.jetty:jetty-util:6.1.26\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: com.sun.jersey:jersey-core:1.9\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: com.sun.jersey:jersey-client:1.9\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.codehaus.jackson:jackson-jaxrs:1.9.13\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.codehaus.jackson:jackson-xc:1.9.13\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.apache.hadoop:hadoop-mapreduce-client-jobclient:2.9.0\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.apache.hadoop:hadoop-annotations:2.9.0\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.apache.spark:spark-streaming-kafka_2.11:1.6.2\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.apache.kafka:kafka_2.11:0.8.2.1\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.scala-lang.modules:scala-xml_2.11:1.0.2\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: com.yammer.metrics:metrics-core:2.2.0\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.scala-lang.modules:scala-parser-combinators_2.11:1.0.2\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: com.101tec:zkclient:0.3\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: org.apache.kafka:kafka-clients:0.8.2.1\" level=\"project\" />\n+    <orderEntry type=\"library\" name=\"Maven: com.alibaba:fastjson:1.2.46\" level=\"project\" />\n+    <orderEntry type=\"library\" scope=\"PROVIDED\" name=\"Maven: mysql:mysql-connector-java:5.1.44\" level=\"project\" />\n+  </component>\n+</module>\n\\ No newline at end of file" }, { "sha" : "74446ca035d38abfdcc01ee446877ba6bfb755a7", "filename" : "spark-study-sql-java/src/main/java/com/wen/spark/sql/core/Hive/HiveDataSource.java", "status" : "added", "additions" : 53, "deletions" : 0, "changes" : 53, "blob_url" : "https://github.com/haha174/spark/blob/d4479ae679dd4b8695e6fad3ac2657d1e94958af/spark-study-sql-java/src/main/java/com/wen/spark/sql/core/Hive/HiveDataSource.java", "raw_url" : "https://github.com/haha174/spark/raw/d4479ae679dd4b8695e6fad3ac2657d1e94958af/spark-study-sql-java/src/main/java/com/wen/spark/sql/core/Hive/HiveDataSource.java", "contents_url" : "https://api.github.com/repos/haha174/spark/contents/spark-study-sql-java/src/main/java/com/wen/spark/sql/core/Hive/HiveDataSource.java?ref=d4479ae679dd4b8695e6fad3ac2657d1e94958af", "patch" : "@@ -0,0 +1,53 @@\n+package com.wen.spark.sql.core.Hive;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SQLContext;\n+import org.apache.spark.sql.hive.HiveContext;\n+\n+public class HiveDataSource {\n+    public static void main(String[] args) {\n+        SparkConf conf=new SparkConf().setAppName(\"HiveDataSource\");\n+        JavaSparkContext sc=new JavaSparkContext(conf);\n+        // 创建HiveContext  注意这里接收的是SparkContext   不是 JavaSparkContext\n+        HiveContext sqlContext=new HiveContext(sc.sc());\n+        //第一个功能，使用HiveContext的Sql()/Hql\n+        sqlContext.sql(\"DROP TABLE IF EXISTS student_info\");\n+\n+       sqlContext.sql(\"CREATE  TABLE IF NOT EXISTS student_info (name STRING ,age INT)\");\n+        System.out.println(\"============================create table success\");\n+        //将学生的基本信息导入到StudentInfo  表\n+        sqlContext.sql(\"LOAD DATA LOCAL INPATH '/data/hive/student_info/student_info.txt' INTO TABLE  student_info\");\n+\n+\n+\n+\n+        sqlContext.sql(\"DROP TABLE IF EXISTS student_scores\");\n+\n+       sqlContext.sql(\"CREATE  TABLE IF NOT EXISTS student_scores (name STRING ,score INT)\");\n+        //将学生的基本分数导入到StudentInfo  表\n+        sqlContext.sql(\"LOAD DATA LOCAL INPATH '/data/hive/student_info/student_scores.txt' INTO TABLE  student_scores\");\n+        //第二个功能接着将sql  返回的DataFrame  用于查询\n+        //执行sql  关联两张表查询大于80分的学生\n+        Dataset goodStudentDS=sqlContext.sql(\"SELECT ss.name ,s1.age,ss.score from student_info s1 JOIN  student_scores ss ON s1.name=ss.name WHERE   ss.score>=80\");\n+\n+\n+\n+        //第三个功能，可以将 DataFrame  中的数据 理论上来说DataFrame  对应的RDD  数据  是ROW  即可\n+        //将DataFrame  保存到Hive  表中·\n+        //  接着将数据保存到good_student_info  中\n+        sqlContext.sql(\"DROP TABLE IF EXISTS good_student_info\");\n+        System.out.println(\"create table success\");\n+        goodStudentDS.write().saveAsTable(\"good_student_info\");\n+        //  第四个功能 针对  good_student_info  表  直接创建   DataSet\n+        Dataset<Row> goodStudentDSRows=sqlContext.tables(\"good_student_info\");\n+        goodStudentDSRows.show();\n+//        Row[] goodStudentRows=goodStudentDSRows.collect();\n+//        for (Row goodStudentRow:goodStudentRows){\n+//            System.out.println(goodStudentRow);\n+//        }\n+        sc.close();\n+    }\n+}" }, { "sha" : "cf46a29d535bb7db88969f494bd760d26d8533c2", "filename" : "spark-study-sql-java/src/main/java/com/wen/spark/sql/core/HiveDataSource.java", "status" : "removed", "additions" : 0, "deletions" : 13, "changes" : 13, "blob_url" : "https://github.com/haha174/spark/blob/8fb03c2f9d84f732ec11ffed9987634188d47e7d/spark-study-sql-java/src/main/java/com/wen/spark/sql/core/HiveDataSource.java", "raw_url" : "https://github.com/haha174/spark/raw/8fb03c2f9d84f732ec11ffed9987634188d47e7d/spark-study-sql-java/src/main/java/com/wen/spark/sql/core/HiveDataSource.java", "contents_url" : "https://api.github.com/repos/haha174/spark/contents/spark-study-sql-java/src/main/java/com/wen/spark/sql/core/HiveDataSource.java?ref=8fb03c2f9d84f732ec11ffed9987634188d47e7d", "patch" : "@@ -1,13 +0,0 @@\n-package com.wen.spark.sql.core;\n-\n-import org.apache.spark.SparkConf;\n-import org.apache.spark.api.java.JavaSparkContext;\n-\n-public class HiveDataSource {\n-    public static void main(String[] args) {\n-        SparkConf conf=new SparkConf().setAppName(\"HiveDataSource\");\n-        JavaSparkContext sc=new JavaSparkContext(conf);\n-\n-\n-    }\n-}" }, { "sha" : "59fd4127cdb7e460fd7d4f7b1a1b819f442393bd", "filename" : "spark-study-sql-java/src/main/java/com/wen/spark/sql/core/jdbc/JdbcDataSource.java", "status" : "added", "additions" : 121, "deletions" : 0, "changes" : 121, "blob_url" : "https://github.com/haha174/spark/blob/d4479ae679dd4b8695e6fad3ac2657d1e94958af/spark-study-sql-java/src/main/java/com/wen/spark/sql/core/jdbc/JdbcDataSource.java", "raw_url" : "https://github.com/haha174/spark/raw/d4479ae679dd4b8695e6fad3ac2657d1e94958af/spark-study-sql-java/src/main/java/com/wen/spark/sql/core/jdbc/JdbcDataSource.java", "contents_url" : "https://api.github.com/repos/haha174/spark/contents/spark-study-sql-java/src/main/java/com/wen/spark/sql/core/jdbc/JdbcDataSource.java?ref=d4479ae679dd4b8695e6fad3ac2657d1e94958af", "patch" : "@@ -0,0 +1,121 @@\n+package com.wen.spark.sql.core.jdbc;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.function.Function;\n+import org.apache.spark.api.java.function.PairFunction;\n+import org.apache.spark.api.java.function.VoidFunction;\n+import org.apache.spark.sql.RowFactory;\n+import org.apache.spark.sql.SQLContext;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+import scala.Tuple2;\n+import org.apache.spark.sql.Row;\n+\n+import java.sql.*;\n+import java.util.*;\n+\n+\n+public class JdbcDataSource {\n+    public static void main(String[] args) {\n+        SparkConf conf=new SparkConf().setAppName(\"JdbcDataSource\");\n+        JavaSparkContext sc=new JavaSparkContext(conf);\n+        SQLContext sqlContext=new SQLContext(sc);\n+        //在两张表中分别取出  转换为  Dataset\n+        Map<String,String> options=new HashMap<String,String>();\n+        options.put(\"url\",\"jdbc:mysql://haha174:3306/test\");\n+        options.put(\"dbtable\",\"students_infos\");\n+        options.put(\"driver\", \"com.mysql.jdbc.Driver\");\n+        options.put(\"user\",\"root\");\n+        options.put(\"password\",\"root\");\n+        Dataset studentsDS=sqlContext.read().format(\"jdbc\").options(options).load();\n+        options.clear();\n+        options.put(\"url\",\"jdbc:mysql://haha174:3306/test\");\n+        options.put(\"dbtable\",\"students_scores\");\n+        options.put(\"driver\", \"com.mysql.jdbc.Driver\");\n+        options.put(\"user\",\"root\");\n+        options.put(\"password\",\"root\");\n+        Dataset scoreDS =sqlContext.read().format(\"jdbc\").options(options).load();\n+        //将两个DataSet  转换为JavaRDD\n+        JavaPairRDD<String,Tuple2<Integer,Integer>> studentRDD=studentsDS.javaRDD().mapToPair(new PairFunction<Row ,String,Integer>() {\n+            private static final long serialVersionUID=1L;\n+            @Override\n+            public Tuple2<String,Integer> call(Row row) throws Exception {\n+                return new Tuple2<String, Integer>(row.getString(1),row.getInt(2));\n+            }\n+        }).join(scoreDS.javaRDD().mapToPair(new PairFunction<Row ,String,Integer>() {\n+            private static final long serialVersionUID=1L;\n+\n+            public Tuple2<String,Integer> call(Row row) throws Exception {\n+                return new Tuple2<String, Integer>(row.getString(1),row.getInt(2));\n+            }\n+        }));\n+\n+        //将JavaRDD  转换为JavaRDD<Row>\n+        JavaRDD<Row> StudentRowRDD=studentRDD.map(new Function<Tuple2<String, Tuple2<Integer, Integer>>, Row>() {\n+            @Override\n+            public Row call(Tuple2<String, Tuple2<Integer, Integer>> stringTuple2Tuple2) throws Exception {\n+                return RowFactory.create(stringTuple2Tuple2._1,stringTuple2Tuple2._2._1,stringTuple2Tuple2._2._2);\n+            }\n+        });\n+        JavaRDD<Row> StudentRowRDDS=  StudentRowRDD.filter(new Function<Row, Boolean>() {\n+            @Override\n+            public Boolean call(Row row) throws Exception {\n+                System.out.println(\"=======================================\"+row.toString());\n+                if(row.getInt(2)>80)\n+                    return true;\n+\n+                return false;\n+            }\n+        });\n+        List<StructField> structFieldList=new ArrayList<StructField>();\n+        structFieldList.add(DataTypes.createStructField(\"name\",DataTypes.StringType,true));\n+        structFieldList.add(DataTypes.createStructField(\"age\",DataTypes.IntegerType,true));\n+        structFieldList.add(DataTypes.createStructField(\"score\",DataTypes.IntegerType,true));\n+        StructType structType=DataTypes.createStructType(structFieldList);\n+        Dataset studentRe=sqlContext.createDataFrame(StudentRowRDDS,structType);\n+        options.clear();\n+        options.put(\"url\",\"jdbc:mysql://haha174:3306/test\");\n+        options.put(\"dbtable\",\"good_students_infos\");\n+        options.put(\"driver\", \"com.mysql.jdbc.Driver\");\n+        options.put(\"user\",\"root\");\n+        options.put(\"password\",\"root\");\n+   //     studentRe.write().format(\"jdbc\").options(options).save();\n+        System.out.println(\"       ===================\"+StudentRowRDDS.count());\n+\n+        StudentRowRDDS.foreach(new VoidFunction<Row>() {\n+            private static final long serialVersionUID=1L;\n+            @Override\n+            public void call(Row o) throws Exception {\n+\n+                Connection connection=null;\n+                Statement statement=null;\n+                try {\n+                    Class.forName(\"com.mysql.jdbc.driver\");\n+                    connection= DriverManager.getConnection(\"jdbc:mysql://haha174:3306/test\",\"root\",\"root\");\n+                    String sql=\"INSERT IN good_students_infos(name,age,score)values(\"+o.getString(0)+\",\"+o.getInt(1)+\",\"+o.getInt(2)+\")\";\n+                    System.out.println(sql);\n+                    statement=connection.createStatement();\n+                    statement.executeUpdate(sql);\n+                } catch (ClassNotFoundException e) {\n+                    e.printStackTrace();\n+                }\n+                catch (SQLException e ){\n+                    e.printStackTrace();\n+                }finally {\n+                    if(connection!=null){\n+                        connection.close();\n+                    }\n+                    if(statement!=null){\n+                        statement.close();\n+                    }\n+                }\n+\n+            }\n+        });\n+    }\n+}" }, { "sha" : "456c63be9bb325d13f070c922f23f5ff4f1b5c50", "filename" : "spark-study-sql-scala/src/main/java/com/wen/spark/sql/core/DataFrameStudy.scala", "status" : "modified", "additions" : 1, "deletions" : 0, "changes" : 1, "blob_url" : "https://github.com/haha174/spark/blob/d4479ae679dd4b8695e6fad3ac2657d1e94958af/spark-study-sql-scala/src/main/java/com/wen/spark/sql/core/DataFrameStudy.scala", "raw_url" : "https://github.com/haha174/spark/raw/d4479ae679dd4b8695e6fad3ac2657d1e94958af/spark-study-sql-scala/src/main/java/com/wen/spark/sql/core/DataFrameStudy.scala", "contents_url" : "https://api.github.com/repos/haha174/spark/contents/spark-study-sql-scala/src/main/java/com/wen/spark/sql/core/DataFrameStudy.scala?ref=d4479ae679dd4b8695e6fad3ac2657d1e94958af", "patch" : "@@ -12,5 +12,6 @@ object DataFrameStudy {\n      val reader = sqlContext.read\n       val ds = reader.json(\"hdfs://hadoop:8020/data/students.json\")\n       ds.show()\n+\n   }\n }" }, { "sha" : "ce8705fc66cb2d8dd347f33c372c1d54c6d355da", "filename" : "spark-study-sql-scala/src/main/java/com/wen/spark/sql/core/Hive/HiveDataSource.scala", "status" : "added", "additions" : 40, "deletions" : 0, "changes" : 40, "blob_url" : "https://github.com/haha174/spark/blob/d4479ae679dd4b8695e6fad3ac2657d1e94958af/spark-study-sql-scala/src/main/java/com/wen/spark/sql/core/Hive/HiveDataSource.scala", "raw_url" : "https://github.com/haha174/spark/raw/d4479ae679dd4b8695e6fad3ac2657d1e94958af/spark-study-sql-scala/src/main/java/com/wen/spark/sql/core/Hive/HiveDataSource.scala", "contents_url" : "https://api.github.com/repos/haha174/spark/contents/spark-study-sql-scala/src/main/java/com/wen/spark/sql/core/Hive/HiveDataSource.scala?ref=d4479ae679dd4b8695e6fad3ac2657d1e94958af", "patch" : "@@ -0,0 +1,40 @@\n+package com.wen.spark.sql.core\n+\n+import org.apache.spark.{SparkConf, SparkContext}\n+import org.apache.spark.api.java.JavaSparkContext\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.hive.HiveContext\n+\n+object HiveDataSource {\n+  def main(args: Array[String]): Unit = {\n+    val conf = new SparkConf().setAppName(\"HiveDataSource\")\n+    val sc = new SparkContext(conf)\n+    // 创建HiveContext  注意这里接收的是SparkContext   不是 JavaSparkContext\n+    val sqlContext = new HiveContext(sc)\n+    //第一个功能，使用HiveContext的Sql()/Hql\n+    sqlContext.sql(\"DROP TABLE IF EXISTS student_info\")\n+    sqlContext.sql(\"CREATE  TABLE IF NOT EXISTS student_info (name STRING ,age INT)\")\n+    System.out.println(\"============================create table success\")\n+    //将学生的基本信息导入到StudentInfo  表\n+    sqlContext.sql(\"LOAD DATA LOCAL INPATH '/data/hive/student_info/student_info.txt' INTO TABLE  student_info\")\n+    sqlContext.sql(\"DROP TABLE IF EXISTS student_scores\")\n+    sqlContext.sql(\"CREATE  TABLE IF NOT EXISTS student_scores (name STRING ,score INT)\")\n+    //将学生的基本分数导入到StudentInfo  表\n+    sqlContext.sql(\"LOAD DATA LOCAL INPATH '/data/hive/student_info/student_scores.txt' INTO TABLE  student_scores\")\n+    //第二个功能接着将sql  返回的DataFrame  用于查询\n+    //执行sql  关联两张表查询大于80分的学生\n+    val goodStudentDS = sqlContext.sql(\"SELECT ss.name ,s1.age,ss.score from student_info s1 JOIN  student_scores ss ON s1.name=ss.name WHERE   ss.score>=80\")\n+    //第三个功能，可以将 DataFrame  中的数据 理论上来说DataFrame  对应的RDD  数据  是ROW  即可\n+    //将DataFrame  保存到Hive  表中·\n+    //  接着将数据保存到good_student_info  中\n+    sqlContext.sql(\"DROP TABLE IF EXISTS good_student_info\")\n+    System.out.println(\"create table success\")\n+    goodStudentDS.write.saveAsTable(\"good_student_info\")\n+    //  第四个功能 针对  good_student_info  表  直接创建   DataSet\n+    val goodStudentDSRows = sqlContext.tables(\"good_student_info\")\n+    val goodStudentRows = goodStudentDSRows.collect\n+    for (goodStudentRow <- goodStudentRows) {\n+      System.out.println(goodStudentRow)\n+    }\n+  }\n+}" }, { "sha" : "75a9cacd09cd03f3724e4d3e040d989f7641591c", "filename" : "spark-study-sql-scala/src/main/java/com/wen/spark/sql/core/ParquetLoadData.scala", "status" : "modified", "additions" : 12, "deletions" : 5, "changes" : 17, "blob_url" : "https://github.com/haha174/spark/blob/d4479ae679dd4b8695e6fad3ac2657d1e94958af/spark-study-sql-scala/src/main/java/com/wen/spark/sql/core/ParquetLoadData.scala", "raw_url" : "https://github.com/haha174/spark/raw/d4479ae679dd4b8695e6fad3ac2657d1e94958af/spark-study-sql-scala/src/main/java/com/wen/spark/sql/core/ParquetLoadData.scala", "contents_url" : "https://api.github.com/repos/haha174/spark/contents/spark-study-sql-scala/src/main/java/com/wen/spark/sql/core/ParquetLoadData.scala?ref=d4479ae679dd4b8695e6fad3ac2657d1e94958af", "patch" : "@@ -2,9 +2,9 @@ package com.wen.spark.sql.core\n \n import org.apache.spark.SparkConf\n import org.apache.spark.api.java.JavaSparkContext\n-import org.apache.spark.sql.DataFrameReader\n-import org.apache.spark.sql.Dataset\n-import org.apache.spark.sql.SQLContext\n+import org.apache.spark.api.java.function.Function\n+import org.apache.spark.sql._\n+\n import scala.collection.immutable.List\n object ParquetLoadData {\n   def main(args: Array[String]): Unit = {\n@@ -16,9 +16,16 @@ object ParquetLoadData {\n     ds.show()\n     ds.registerTempTable(\"users\")\n     val userName = sqlContext.sql(\"select name from users\")\n-    //对查询出来的dataSet 进行操作，处理打印\n-    val userNameRDD = userName.javaRDD.map(line=>\"name:\"+line.getString(0)).collect\n+    //RDD<ROW>\n     import scala.collection.JavaConversions._\n+    //对查询出来的dataSet 进行操作，处理打印\n+  val userNameRDD = userName.javaRDD.map(new Function[Row, String]() {\n+    @throws[Exception]\n+    override def call(o: Row): String = {\n+      return \"name:\" + o.getString(0)\n+    }\n+  }).collect()\n+\n     for (usernmae <- userNameRDD) {\n       println(usernmae)\n     }" }, { "sha" : "0d2cebb87515670b1e151b4a907771aa8800b3a7", "filename" : "spark-study-sql-scala/src/main/java/com/wen/spark/sql/core/RDDToDataSetProgrammatically.scala", "status" : "modified", "additions" : 2, "deletions" : 1, "changes" : 3, "blob_url" : "https://github.com/haha174/spark/blob/d4479ae679dd4b8695e6fad3ac2657d1e94958af/spark-study-sql-scala/src/main/java/com/wen/spark/sql/core/RDDToDataSetProgrammatically.scala", "raw_url" : "https://github.com/haha174/spark/raw/d4479ae679dd4b8695e6fad3ac2657d1e94958af/spark-study-sql-scala/src/main/java/com/wen/spark/sql/core/RDDToDataSetProgrammatically.scala", "contents_url" : "https://api.github.com/repos/haha174/spark/contents/spark-study-sql-scala/src/main/java/com/wen/spark/sql/core/RDDToDataSetProgrammatically.scala?ref=d4479ae679dd4b8695e6fad3ac2657d1e94958af", "patch" : "@@ -15,13 +15,14 @@ object RDDToDataSetProgrammatically {\n     val conf = new SparkConf().setMaster(\"local\").setAppName(\"RDDToDataSetReflection\")\n     val sc = new JavaSparkContext(conf)\n     val sqlContext = new SQLContext(sc)\n+    import sqlContext.implicits. _\n     val listRDD = sc.textFile(\"C:\\\\Users\\\\wchen129\\\\Desktop\\\\data\\\\sparkdata\\\\students.txt\")\n     //第一步创建RDD  但是需要转换成RDD<Row>\n     val rowRDD = listRDD.map(new Function[String, Row]() {\n       @throws[Exception]\n       override def call(s: String): Row = {\n         val string = s.split(\",\")\n-        RowFactory.create(string(0).toInt, string(1), string(2).toInt)\n+        Row(Integer.parseInt(string(0)), string(1),Integer.parseInt( string(2)))\n       }\n     })\n     //动态构建元数据" }, { "sha" : "5f6113d5a3a1917f889bf2aa43764430e7c6e8d1", "filename" : "spark-study-sql-scala/src/main/java/com/wen/spark/sql/core/RDDToDataSetReflection.scala", "status" : "modified", "additions" : 10, "deletions" : 6, "changes" : 16, "blob_url" : "https://github.com/haha174/spark/blob/d4479ae679dd4b8695e6fad3ac2657d1e94958af/spark-study-sql-scala/src/main/java/com/wen/spark/sql/core/RDDToDataSetReflection.scala", "raw_url" : "https://github.com/haha174/spark/raw/d4479ae679dd4b8695e6fad3ac2657d1e94958af/spark-study-sql-scala/src/main/java/com/wen/spark/sql/core/RDDToDataSetReflection.scala", "contents_url" : "https://api.github.com/repos/haha174/spark/contents/spark-study-sql-scala/src/main/java/com/wen/spark/sql/core/RDDToDataSetReflection.scala?ref=d4479ae679dd4b8695e6fad3ac2657d1e94958af", "patch" : "@@ -2,6 +2,7 @@ package com.wen.spark.sql.core\n \n import java.util\n \n+import com.alibaba.fastjson.JSONObject\n import com.wen.spark.sql.core.bean.Student\n import org.apache.spark.SparkConf\n import org.apache.spark.api.java.{JavaRDD, JavaSparkContext}\n@@ -14,15 +15,18 @@ object  RDDToDataSetReflection {\n     val sc = new JavaSparkContext(conf)\n     val sqlContext = new SQLContext(sc)\n \n-    val listRDD = sc.textFile(\"C:\\\\Users\\\\wchen129\\\\Desktop\\\\data\\\\sparkdata\\\\students.txt\")\n+    val listRDD = sc.textFile(\"C:\\\\Users\\\\wchen129\\\\Desktop\\\\data\\\\sparkdata\\\\students.json\")\n     val listStudent = listRDD.map(new Function[String, Student]() {\n       @throws[Exception]\n       override def call(s: String): Student = {\n-        var string=s.split(\",\")\n-        var student=new Student;\n-        student.setId(Integer.valueOf(string(0).trim()))\n-        student.setAge(Integer.valueOf(string(2).trim()))\n-        student.setName(string(1))\n+\n+        val student = com.alibaba.fastjson.JSON.parseObject(s, classOf[Student])\n+\n+//        var string=s.split(\",\")\n+//        var student=new Student;\n+//        student.setId(Integer.valueOf(string(0).trim()))\n+//        student.setAge(Integer.valueOf(string(2).trim()))\n+//        student.setName(string(1))\n         student\n       }\n     })" }, { "sha" : "38da907be20890b8360494cf12b4f3d11139df90", "filename" : "spark-study-sql-scala/src/main/java/com/wen/spark/sql/core/UDF.scala", "status" : "added", "additions" : 29, "deletions" : 0, "changes" : 29, "blob_url" : "https://github.com/haha174/spark/blob/d4479ae679dd4b8695e6fad3ac2657d1e94958af/spark-study-sql-scala/src/main/java/com/wen/spark/sql/core/UDF.scala", "raw_url" : "https://github.com/haha174/spark/raw/d4479ae679dd4b8695e6fad3ac2657d1e94958af/spark-study-sql-scala/src/main/java/com/wen/spark/sql/core/UDF.scala", "contents_url" : "https://api.github.com/repos/haha174/spark/contents/spark-study-sql-scala/src/main/java/com/wen/spark/sql/core/UDF.scala?ref=d4479ae679dd4b8695e6fad3ac2657d1e94958af", "patch" : "@@ -0,0 +1,29 @@\n+package com.wen.spark.sql.core\n+\n+import org.apache.spark.sql.types.{DataTypes, StructType}\n+import org.apache.spark.sql.{Row, SQLContext}\n+import org.apache.spark.{SparkConf, SparkContext}\n+\n+object UDF {\n+  def main(args: Array[String]): Unit = {\n+    val conf=new SparkConf().setMaster(\"local\").setAppName(\"UDF\")\n+    val sc=new SparkContext(conf)\n+    val sqlContext=new SQLContext(sc)\n+    //  构造模拟数据\n+    val names=Array(\"Leo\",\"Marry\",\"Jack\",\"Tom\");\n+    var namesRDD=sc.parallelize(names);\n+    var namesRowRDD=namesRDD.map(name=>Row(name))\n+    var structType=StructType(Array(\n+      DataTypes.createStructField(\"name\", DataTypes.StringType, true)\n+    ))\n+    val namesDF=sqlContext.createDataFrame(namesRowRDD,structType)\n+    //注册一张表\n+    namesDF.registerTempTable(\"names\")\n+    //自定义函数\n+    sqlContext.udf.register(\"strLen\",(str:String)=>str.length)\n+   val namesREDF=   sqlContext.sql(\"select name,strLen(name) from names\")\n+    namesREDF   .collect().foreach(row=>println(row.getString(0)+\"  \"+row.getInt(1)))\n+\n+    namesREDF.show()\n+  }\n+}" }, { "sha" : "649aa0cb7be232c2467fc52f4bbc5f35638e557d", "filename" : "spark-study-sql-scala/src/main/java/com/wen/spark/sql/core/UDFFunction/StringCount.scala", "status" : "added", "additions" : 42, "deletions" : 0, "changes" : 42, "blob_url" : "https://github.com/haha174/spark/blob/d4479ae679dd4b8695e6fad3ac2657d1e94958af/spark-study-sql-scala/src/main/java/com/wen/spark/sql/core/UDFFunction/StringCount.scala", "raw_url" : "https://github.com/haha174/spark/raw/d4479ae679dd4b8695e6fad3ac2657d1e94958af/spark-study-sql-scala/src/main/java/com/wen/spark/sql/core/UDFFunction/StringCount.scala", "contents_url" : "https://api.github.com/repos/haha174/spark/contents/spark-study-sql-scala/src/main/java/com/wen/spark/sql/core/UDFFunction/StringCount.scala?ref=d4479ae679dd4b8695e6fad3ac2657d1e94958af", "patch" : "@@ -0,0 +1,42 @@\n+package com.wen.spark.sql.core.UDFFunction\n+\n+import org.apache.avro.generic.GenericData.StringType\n+import org.apache.spark.sql.Row\n+import org.apache.spark.sql.expressions.{MutableAggregationBuffer, UserDefinedAggregateFunction}\n+import org.apache.spark.sql.types._\n+\n+\n+class StringCount extends UserDefinedAggregateFunction{\n+  //输入数据的类型\n+  override def inputSchema: StructType = {\n+    StructType(Array(StructField(\"str\",org.apache.spark.sql.types.StringType,true)))\n+  }\n+  //中间聚合时所处理的数据的类型\n+  override def bufferSchema: StructType={\n+    StructType(Array(StructField(\"count\",org.apache.spark.sql.types.IntegerType,true)))\n+  }\n+  // 函数返回值的类型\n+  override def dataType: DataType = {\n+    org.apache.spark.sql.types.IntegerType\n+  }\n+\n+  override def deterministic: Boolean = true\n+//为每个分组的数据执行初始化操作\n+  override def initialize(buffer: MutableAggregationBuffer): Unit = {\n+    buffer(0)=0\n+  }\n+  //每个分组有一个新的值进来的时候如何进行分组对应的额聚合计算\n+  override def update(buffer: MutableAggregationBuffer, input: Row): Unit = {\n+    buffer(0)=buffer.getAs[Int](0)+1\n+  }\n+\n+  //由于spark  是分布式，所以一个分组的数组会在不同的节点上进行聚合，就是update\n+  //但是最后一个分组在各个节点上的聚合值进行合并\n+  override def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = {\n+    buffer1(0)=buffer1.getAs[Int](0)+buffer2.getAs[Int](0)\n+  }\n+  //一个分组的聚合值，如何通过中间的缓存聚合值，最后返回一个最终的聚合值\n+  override def evaluate(buffer: Row): Any = {\n+    buffer.getAs[Int](0)\n+  }\n+}" }, { "sha" : "794794a796f52a05caeee08f197e0a45b912bc4b", "filename" : "spark-study-sql-scala/src/main/java/com/wen/spark/sql/core/UDFFunction/UDFCount.scala", "status" : "added", "additions" : 28, "deletions" : 0, "changes" : 28, "blob_url" : "https://github.com/haha174/spark/blob/d4479ae679dd4b8695e6fad3ac2657d1e94958af/spark-study-sql-scala/src/main/java/com/wen/spark/sql/core/UDFFunction/UDFCount.scala", "raw_url" : "https://github.com/haha174/spark/raw/d4479ae679dd4b8695e6fad3ac2657d1e94958af/spark-study-sql-scala/src/main/java/com/wen/spark/sql/core/UDFFunction/UDFCount.scala", "contents_url" : "https://api.github.com/repos/haha174/spark/contents/spark-study-sql-scala/src/main/java/com/wen/spark/sql/core/UDFFunction/UDFCount.scala?ref=d4479ae679dd4b8695e6fad3ac2657d1e94958af", "patch" : "@@ -0,0 +1,28 @@\n+package com.wen.spark.sql.core.UDFFunction\n+\n+import org.apache.spark.sql.types.{DataTypes, StructField, StructType}\n+import org.apache.spark.sql.{Row, SQLContext}\n+import org.apache.spark.{SparkConf, SparkContext}\n+\n+object UDFCount {\n+  def main(args: Array[String]): Unit = {\n+    val conf=new SparkConf().setMaster(\"local\").setAppName(\"UDF\")\n+    val sc=new SparkContext(conf)\n+    val sqlContext=new SQLContext(sc)\n+    //  构造模拟数据\n+    val names=Array(\"Leo\",\"Marry\",\"Jack\",\"Tom\",\"Leo\",\"Marry\",\"Jack\",\"Tom\",\"Leo\",\"Marry\",\"Jack\",\"Tom\");\n+    val namesRDD=sc.parallelize(names);\n+    val namesRowRDD=namesRDD.map(name=>Row(name))\n+    val structType= StructType(Array(StructField(\"name\",org.apache.spark.sql.types.StringType,true)))\n+\n+    var namesDF=sqlContext.createDataFrame(namesRowRDD,structType)\n+    //注册一张表\n+    namesDF.registerTempTable(\"names\")\n+    //自定义函数\n+    sqlContext.udf.register(\"strCount\",new StringCount())\n+   val namesREDF=   sqlContext.sql(\"select name,strCount(name) from names group by name\")\n+    namesREDF   .collect().foreach(row=>println(row.getString(0)+\"  \"+row.getInt(1)))\n+\n+    namesREDF.show()\n+  }\n+}" }, { "sha" : "768cd7cc2e613c0508fcc243eab2d3a67eb4c779", "filename" : "spark-study-sql-scala/src/main/java/com/wen/spark/sql/core/function/DailySale.scala", "status" : "added", "additions" : 57, "deletions" : 0, "changes" : 57, "blob_url" : "https://github.com/haha174/spark/blob/d4479ae679dd4b8695e6fad3ac2657d1e94958af/spark-study-sql-scala/src/main/java/com/wen/spark/sql/core/function/DailySale.scala", "raw_url" : "https://github.com/haha174/spark/raw/d4479ae679dd4b8695e6fad3ac2657d1e94958af/spark-study-sql-scala/src/main/java/com/wen/spark/sql/core/function/DailySale.scala", "contents_url" : "https://api.github.com/repos/haha174/spark/contents/spark-study-sql-scala/src/main/java/com/wen/spark/sql/core/function/DailySale.scala?ref=d4479ae679dd4b8695e6fad3ac2657d1e94958af", "patch" : "@@ -0,0 +1,57 @@\n+package com.wen.spark.sql.core.function\n+\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types.{DataTypes, StructType}\n+import org.apache.spark.sql.{Row, SQLContext}\n+import org.apache.spark.{SparkConf, SparkContext}\n+\n+object DailySale {\n+  def main(args: Array[String]): Unit = {\n+    val conf=new SparkConf().setAppName(\"DailyUV\").setMaster(\"local\")\n+    val  sc = new SparkContext(conf)\n+    val sqlContext = new SQLContext(sc)\n+    //1  模拟用户行为日志  第一列是时间，第二列是用户id\n+    val userAccessLog=Array(\n+      \"2015-10-01,1122,1.0\",\n+      \"2015-10-01,1122,2.0\",\n+      \"2015-10-01,1123,3.0\",\n+      \"2015-10-01,1124,4.0\",\n+      \"2015-10-02,1121,5.0\",\n+      \"2015-10-02,1122,6.0\",\n+      \"2015-10-01,1123,7.0\",\n+      \"2015-10-01,1124,8.0\");\n+    //2  创建RDD\n+    val userAccessLogRDD=sc.parallelize(userAccessLog,5);\n+    //3 将RDD  转换为DataSet 首先转换元素为Row 的RDD\n+    val userAccessLogRowRDD=userAccessLogRDD.map(log=>{\n+      Row(log.split(\",\")(0),log.split(\",\")(1).toInt,log.split(\",\")(2).toDouble)\n+    })\n+    //然后构造DataSet元数据\n+    val structType=StructType(Array(\n+      DataTypes.createStructField(\"date\", DataTypes.StringType, true),\n+      DataTypes.createStructField(\"userId\", DataTypes.IntegerType, true),\n+      DataTypes.createStructField(\"sale\", DataTypes.DoubleType, true)\n+    ))\n+    //使用sqlContext  创建DataSet\n+    import sqlContext.implicits._\n+    val userAccessLogRowDS=sqlContext.createDataFrame( userAccessLogRowRDD,structType);\n+    //统计每日的销售额\n+    userAccessLogRowDS.groupBy(\"date\")\n+      .agg('date,sum(\"sale\"))//DataSet<Row>\n+      .show()\n+\n+\n+\n+    //统计每个用户的销售额\n+    userAccessLogRowDS.groupBy(\"userId\")\n+      .agg('userId,sum(\"sale\"))//DataSet<Row>\n+      .show()\n+\n+\n+    //统计每个用户每日的销售额\n+    userAccessLogRowDS.groupBy(\"userId\",\"date\")\n+      .agg(sum(\"sale\"))//DataSet<Row>\n+      .show()\n+\n+  }\n+}" }, { "sha" : "8cfc7589e6849742d5372a9e4761f023cca8d83d", "filename" : "spark-study-sql-scala/src/main/java/com/wen/spark/sql/core/function/DailyUV.scala", "status" : "added", "additions" : 44, "deletions" : 0, "changes" : 44, "blob_url" : "https://github.com/haha174/spark/blob/d4479ae679dd4b8695e6fad3ac2657d1e94958af/spark-study-sql-scala/src/main/java/com/wen/spark/sql/core/function/DailyUV.scala", "raw_url" : "https://github.com/haha174/spark/raw/d4479ae679dd4b8695e6fad3ac2657d1e94958af/spark-study-sql-scala/src/main/java/com/wen/spark/sql/core/function/DailyUV.scala", "contents_url" : "https://api.github.com/repos/haha174/spark/contents/spark-study-sql-scala/src/main/java/com/wen/spark/sql/core/function/DailyUV.scala?ref=d4479ae679dd4b8695e6fad3ac2657d1e94958af", "patch" : "@@ -0,0 +1,44 @@\n+package com.wen.spark.sql.core.function\n+\n+import org.apache.avro.generic.GenericData.StringType\n+import org.apache.spark.api.java.function.Function\n+import org.apache.spark.sql.types.{DataTypes, IntegerType, StructField, StructType}\n+import org.apache.spark.{SparkConf, SparkContext}\n+import org.apache.spark.sql.{Row, RowFactory, SQLContext}\n+import org.apache.spark.sql.functions._\n+object DailyUV {\n+  def main(args: Array[String]): Unit = {\n+    val conf=new SparkConf().setAppName(\"DailyUV\").setMaster(\"local\")\n+    val  sc = new SparkContext(conf)\n+    val sqlContext = new SQLContext(sc)\n+    //1  模拟用户行为日志  第一列是时间，第二列是用户id\n+    val userAccessLog=Array(\n+      \"2015-10-01,1122\",\n+      \"2015-10-01,1122\",\n+      \"2015-10-01,1123\",\n+      \"2015-10-01,1124\",\n+      \"2015-10-02,1121\",\n+      \"2015-10-02,1122\",\n+      \"2015-10-01,1123\",\n+      \"2015-10-01,1124\");\n+    //2  创建RDD\n+    val userAccessLogRDD=sc.parallelize(userAccessLog,5);\n+    //3 将RDD  转换为DataSet 首先转换元素为Row 的RDD\n+    val userAccessLogRowRDD=userAccessLogRDD.map(log=>{\n+      Row(log.split(\",\")(0),log.split(\",\")(1).toInt)\n+    })\n+    //然后构造DataSet元数据\n+    val structType=StructType(Array(\n+      DataTypes.createStructField(\"date\", DataTypes.StringType, true),\n+      DataTypes.createStructField(\"userId\", DataTypes.IntegerType, true)\n+    ))\n+    //使用sqlContext  创建DataSet\n+    import sqlContext.implicits. _\n+    val userAccessLogRowDS=sqlContext.createDataFrame( userAccessLogRowRDD,structType);\n+    userAccessLogRowDS.groupBy(\"date\")\n+      .agg('date,countDistinct('userId),count('userId))//DataSet<Row>\n+      .show()\n+\n+    userAccessLogRowDS.show()\n+  }\n+}" } ] }"""
  }
  
//  test("Json Extraction Test 01") {
//    new TestJsonData {
//      assert(extract(testJson)(3).commit_timestamp === "2018-04-25T02:03:13Z")
//    }
//  }
  
  test("Language Extraction 01") {
    assert(extractLanguage("hello_world.py") === "Python")
    assert(extractLanguage("     hello_world.hs       ") === "Haskell")
    assert(extractLanguage("hello_world.scala\n") === "Scala")
    assert(extractLanguage("hello_world.java\r\n") === "Java")
    assert(extractLanguage("hello_world.js") === "JavaScript")
  }
  
  test("Python Packages") {
    // Python
    val pythonImports =
      """+from unittest import TestCase as TC, main
        |+import time
        |-import pandas
      """.stripMargin
    assert(extractPackages("Python", pythonImports).toSet === Set((1, "unittest"), (1, "time"), (-1, "pandas")))
  }

  test("Scala Packages") {
    // Scala
    val scalaImports =
      """+      import org.apache.spark
        |       +import org.apache.spark.{SparkConf, SparkContext}
        |+import org.apache.spark.sql.SQLContext
        |-import       org.apache.spark.sql.SparkSession"""
    assert(extractPackages("Scala", scalaImports).toSet ===
      Set(
        (1, "org.apache.spark"),
        (1, "org.apache.spark.sql.SQLContext"),
        (-1, "org.apache.spark.sql.SparkSession")
      )
    )
  }

  test("Java Packages") {
    // Java
    val javaImports =
      """+import static java.awt.Color;
        |+import java.awt.*;
        |-import javax.swing.JOptionPane;"""
    assert(extractPackages("Java", javaImports).toSet ===
      Set(
        (1, "java.awt.Color"),
        (1, "java.awt"),
        (-1, "javax.swing.JOptionPane")
      )
    )
  }

  test("Haskell Packages") {
    // Haskell
    val haskellImports =
      """+import Mod1
        |+import Mod2 (x,y)
        |-import qualified Mod3
        |-import qualified Mod4
        |+import Mod5 hiding (x,y)
        |+import qualified Mod6"""
    assert(extractPackages("Haskell", haskellImports).toSet ===
      Set(
        (1, "Mod1"),
        (1, "Mod2"),
        (-1, "Mod3"),
        (-1, "Mod4"),
        (1, "Mod5"),
        (1, "Mod6")
      )
    )
  }

  test("Rust Packages") {
    val rustImports =
      """+use TrafficLight::*;
        |+use APackage::{Red, Yellow};
      """.stripMargin
    assert(extractPackages("Rust", rustImports).toSet === Set((1, "TrafficLight"), (1, "APackage")))
  }

  test("JavaScript Imports") {
    val javaScriptImports =
      """+import defaultExport from "module-name1";
        |+import * as name from "module-name2";
        |+import { export } from "module-name3";
        |+import { export as alias } from "module-name4";
        |+import { export1 , export2 } from "module-name5";
        |+import { export1 , export2 as alias2 , [...] } from "module-name6";
        |+import defaultExport, { export [ , [...] ] } from "module-name7";
        |+import defaultExport, * as name from "module-name8";
        |+import "module-name9";
      """.stripMargin
    assert(extractPackages("JavaScript", javaScriptImports).toSet === (for (n <- 1 to 9) yield (1, s"module-name$n")).toSet)
  }


  test("Date Extraction") {
    assert(extractDate("2018-04-25T02:03:55Z", "2018-01-01") === "2018-04-25")
    assert(extractDate("2018-04-25T02:01:49Z", "2018-01-01") === "2018-04-25")
    assert(extractDate("2015-01-01T02:03:48Z", "2018-01-01") === "2015-01-01")
    assert(extractDate("MALFORMED", "2018-01-01") === "2018-01-01")
  }
}
